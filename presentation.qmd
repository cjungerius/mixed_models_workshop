---
title: "Mixed models: theory and application"
format: revealjs
author: Chris Jungerius
date: 2024/01/30
date-format: "MMM D, YYYY"
css: styles.css
---

```{r}

library(tidyverse)
library(lme4)
library(brms)
library(emmeans)
library(faux)
library(modelr)
library(patchwork)
library(ggbeeswarm)
library(tidybayes)
library(knitr)
library(kableExtra)

set.seed(90095)

data_1 <- tibble(
  x1 = 1:100,
  x2 = rbinom(100, size = 1, prob = 0.5),
  y1 = rnorm(100, mean = 10, sd = 1),
  y2 = 0.5 * x1 + rnorm(100, mean = 0, sd = 10),
  y3 = x2 + rnorm(100, mean = 0, sd = .5)
)

data_2 <- tibble(
  x = 1:20
) %>% 
  add_random(sub_id=10) %>%
  add_ranef("sub_id",u0s=10, u1s=1, .cors=.2) %>%
  add_ranef(error=10) %>% 
  mutate(
    y = (5+u1s) * x + u0s + error
  )

mixed_example <- lmer(y~x+(x|sub_id),data=data_2)

data_2 <- data_2 %>% add_predictions(mixed_example)

data <- readRDS("mixed_model_data.rds")
no_pool_b <- readRDS("no_pool_b.rds")
ppool_b <- readRDS("ppool_b.rds")

```

```{r}
m_nopool <- lm(y ~ x*sub,data=data)
m_ppool <- lmer(y~ 1 + x + (1 + x | sub),data=data)

data_pred <- data %>% 
  data_grid(sub,x) %>% 
  spread_predictions(m_nopool,m_ppool)

p_data <- data %>% 
  ggplot(
    aes(x=x,y=y,color=sub)
  ) +
  geom_beeswarm(size=3) +
  theme_bw() +
  guides(color='none')+
  facet_grid(.~sub)

p_nopool <- data_pred %>% 
  ggplot(
    aes(x=x,y=m_nopool,color=sub)
  ) +
  stat_summary(fun='mean',geom='point')+
  stat_summary(geom='line',fun='mean',group='none')+
  theme_bw()+
  guides(color='none')+
  facet_grid(.~sub)

p_ppool <- data_pred %>% 
  ggplot(
    aes(x=x,y=m_ppool,color=sub)
  ) +
  stat_summary(fun='mean',geom='point')+
  stat_summary(geom='line',fun='mean',group='none')+
  theme_bw()+
  guides(color='none')+
  facet_grid(.~sub)

p_bayes_coefs <- data %>% 
  data_grid(sub,x) %>% 
  add_predicted_draws(no_pool_b, value="nopool") %>% 
  group_by(sub,x) %>% 
  median_hdi(.simple_names=F) %>% 
  add_predicted_draws(ppool_b, value="ppool") %>% 
  median_hdi(.simple_names=F) %>% 
  ggplot(
    aes(y=interaction(x,sub))
  ) +
  geom_point(aes(x=nopool),position=position_nudge(y=.1)) +
  geom_errorbar(aes(x=nopool,xmin=nopool.lower,xmax=nopool.upper),width=0,position=position_nudge(y=.1))+
  geom_point(aes(x=ppool),color='red',position=position_nudge(y=-.1))+
  geom_errorbar(aes(x=ppool,xmin=ppool.lower,xmax=ppool.upper),width=0,color='red',position=position_nudge(y=-.1))

```


## What's a mixed model?
![](mmmeme.jpeg){fig-align="center"}



## The linear model
$$ 
y = \alpha + \beta x + \epsilon 
$$

. . .

$$
Y = X\mathcal{B} + \epsilon
$$

. . .

$$ 
y \sim Normal(\mu, \sigma)\\
\mu = \alpha + \beta x\\
$$

## $y = \alpha + \epsilon$
```{r}
p1 <- data_1 %>% 
ggplot(aes(x=x1,y=y1))+
geom_point(size=3) +
theme_minimal() +
xlab('X') + 
ylab('Y')

p1
```

## $y = \alpha + \epsilon$
```r
lm(y ~ 1, data=df)
```
```{r}
p1 +
geom_smooth(method='lm', formula = y ~ 1)
```

## $y = \alpha + \beta x + \epsilon$ (continuous)
```{r}
p2 <- data_1 %>% 
ggplot(aes(x=x1,y=y2))+
geom_point(size=3) +
theme_minimal() +
xlab('X') + 
ylab('Y')

p2
```

## $y = \alpha + \beta x + \epsilon$ (continuous)

```r
lm(y ~ 1 + x, data=df)
```
```{r}
p2 + geom_smooth(method='lm', formula=y~1+x)
```

## $y = \alpha + \beta x + \epsilon$ (categorical)
```{r}
p3 <- data_1 %>% 
ggplot(aes(x=x2,y=y3))+
geom_point(size=3) +
theme_minimal() +
xlab('X') + 
ylab('Y') +
scale_x_continuous(breaks=c(0,1), limits=c(-0.5,1.5))

p3
```

## $y = \alpha + \beta x + \epsilon$ (categorical)

```r
lm(y ~ 1 + x, data=df)
```

```{r}
p3 + geom_smooth(method='lm')
```

## But sometimes our data is clustered...

:::{.incremental}
- within participants
- within stimuli types
- participants within groups
- measuring at different times
- measuring at different locations/by different people/using different materials
:::

## But sometimes our data is clustered...
```{r}
p4 <- data_2 %>% 
  ggplot(
    aes(
      x=x,y=y,color=factor(sub_id)
    )
  ) +
  geom_point() +
  theme_minimal() +
  guides(color='none', fill='none')

p4
```

## Full pooling

```r
lm(y ~ 1 + x, data=df)
```
```{r}
p4 +
geom_smooth(aes(group='none'),color='blue',method='lm',formula=y ~ 1 + x)
```

. . .

...ignores variability


## No pooling

```r
lm(y ~ 1 + x + subj + x:subj, data=df) #or y ~ 1 + x * subj
```

```{r}
p4 + geom_smooth(method='lm',se=F)
```

. . .

addresses variability, but...

## No pooling

:::{.incremental}
- less powerful: estimating a different intercept & slope for each group increases df
- likely to overfit to sample
- if clusters are of unequal size, some estimates are better informed than others
- sensitive to outliers
- unclear how results generalise to e.g., population
- *we can do better*
:::

## The solution: partial pooling

:::{.incremental}
- compromise between full and no pooling
- depend on the idea that clusters are *exchangeable*
- model clusters as coming from an *infinitely large population* of clusters (which doesn't have to actually exist!)
- estimate the variance of that constructed 'population'
- reduces overfitting
- less sensitive to outliers
- allow the clusters 'share' information
:::

## ...Partial pooling
```{r}
p4 + geom_smooth(aes(y=pred),method='lm')
```

## Now it's your turn!

Open Rstudio!

## Load in packages

```r
library(tidyverse)
library(lme4)
library(emmeans)
```

## Get data from my repo:

```r
df <- readRDS(url("http://tinyurl.com/mixedmodeldata"))
```

## Let's take a look at the data

```r
view(df)
```

## Let's take a look at the data

```r
view(df)
```



```{r}
kable(data) %>% 
scroll_box(width = "100%", height = "500px")
```


## Let's take a look at the data

```r
df %>% 
group_by(sub, x) %>% 
summarise(y=mean(y), n=n())
```

## Let's take a look at the data

```r
df %>% 
group_by(sub, x) %>% 
summarise(y=mean(y), n=n())
```

```{r}
df <- data
df %>% 
group_by(sub, x) %>% 
summarise(y.m=mean(y), y.sd = sd(y), n=n()) %>% 
kable %>% 
scroll_box(width="100%",height="500px")
```

## Let's take a look at the data

```r
df %>% 
ggplot(
  aes(x=x,y=y,color=sub)
) +
geom_jitter()
```

```{r}
df %>% 
ggplot(
  aes(x=x,y=y,color=sub)
) +
geom_jitter()
```

## Let's take a look at the data

```r
df %>% 
ggplot(
  aes(x=x,y=y,color=sub)
) +
geom_jitter()+
facet_grid(.~sub)
```

```{r}
df %>% 
ggplot(
  aes(x=x,y=y,color=sub)
) +
geom_jitter()+
facet_grid(.~sub)
```

## Fit our models
```r
full_pool <- lm(y ~ x, data=df)       # full pooling
no_pool <- lm(y ~ x * sub, data=df)   # no pooling
```

```{r}
full_pool <- lm(y ~ x, data=df)       # full pooling
no_pool <- lm(y ~ x * sub, data=df)   # no pooling
```

## How do they look?

```r
summary(full_pool)
```

```{r}
summary(full_pool)
```


## How do they look?

```r
full_pool %>% emmeans(~x)
```

```{r}
full_pool %>% emmeans(~x)
```

## How do they look?

```r
summary(no_pool)
```

```{r}
summary(no_pool)
```

## How do they look?

```r
no_pool %>% emmeans(~x) # Doesn't even work because of small clusters!
```

```{r}
no_pool %>% emmeans(~x)
```

## How well do they fit?

```{r}
#| echo: true

df %>%
add_predictions(full_pool) %>% 
ggplot(
  aes(
    x=as.numeric(x), color=sub
  )
) +
stat_summary(aes(y=y)) +
geom_smooth(aes(y=pred),method='lm')
```


## How well do they fit?

```{r}
#| echo: true

df %>%
add_predictions(no_pool) %>% 
ggplot(
  aes(
    x=as.numeric(x), color=sub
  )
) +
stat_summary(aes(y=y)) +
geom_smooth(aes(y=pred),method='lm')
```

## Let's mix it up!

```r
partial_pool <- lmer(y ~ x + (x | sub), data=df) # or 1 + x + (1 + x | sub)
```

```{r}
partial_pool <- lmer(y ~ x + (x | sub), data=df) # or 1 + x + (1 + x | sub)
```

## How does the mix-up look?
```r
summary(partial_pool)
```
```{r}
summary(partial_pool)
```

## How does the mix-up look?
```r
partial_pool %>% emmeans(~x)
```
```{r}
partial_pool %>% emmeans(~x)
```

## How well does the mixup fit?

```{r}
#| echo: true

df %>%
add_predictions(partial_pool) %>% 
ggplot(
  aes(
    x=as.numeric(x), color=sub
  )
) +
stat_summary(aes(y=y)) +
geom_smooth(aes(y=pred),method='lm')
```

## How well does the mixup fit?
```{r}
p_data
```
## How well does the mixup fit?
```{r}
p_nopool
```
## How well does the mixup fit?
```{r}
p_ppool
```


## The mixed model

:::{.incremental}
- recovers the parameters almost as well as the full pooling model
- fits the data almost as well as the no pooling model
- the ideal bias-variance trade-off!
:::

## In addition
:::{.incremental}
- These models are robust against outliers and missing data
- extend to any arbitrary clustering (nested, crossed, etc.)
- have a natural extension to *generalised* linear models through linking functions
:::


## Some words of warning:
:::{.incremental}
- With balanced groups and linear data, mixed models will not magically improve your inferences
- These models will not always fit: singularity, non-convergence
- Interpreting p-values for coefficients becomes difficult/impossible because of effective df reduction: move to differences in group means, likelihood ratio tests vs simpler models, etc. to quantify your model's performance
:::

## The end
...Bayesian bonus?