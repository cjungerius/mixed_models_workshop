---
title: "Mixed models: theory and application"
format: revealjs
author: Chris Jungerius
date: 2024/01/30
date-format: "MMM D, YYYY"
---

```{r}

library(tidyverse)
library(lme4)
library(brms)
library(emmeans)
library(faux)
library(modelr)
library(patchwork)
library(ggbeeswarm)
library(tidybayes)
library(knitr)

set.seed(90095)

data_1 <- tibble(
  x1 = 1:100,
  x2 = rbinom(100, size = 1, prob = 0.5),
  y1 = rnorm(100, mean = 10, sd = 1),
  y2 = 0.5 * x1 + rnorm(100, mean = 0, sd = 10),
  y3 = x2 + rnorm(100, mean = 0, sd = .5)
)

data_2 <- tibble(
  x = 1:20
) %>% 
  add_random(sub_id=10) %>%
  add_ranef("sub_id",u0s=10, u1s=1, .cors=.2) %>%
  add_ranef(error=10) %>% 
  mutate(
    y = (5+u1s) * x + u0s + error
  )

mixed_example <- lmer(y~x+(x|sub_id),data=data_2)

data_2 <- data_2 %>% add_predictions(mixed_example)

data <- readRDS("mixed_model_data.rds")
no_pool_b <- readRDS("no_pool_b.rds")
ppool_b <- readRDS("ppool_b.rds")

```

```{r}
m_nopool <- lm(y ~ x*sub,data=data)
m_ppool <- lmer(y~ 1 + x + (1 + x | sub),data=data)

data_pred <- data %>% 
  data_grid(sub,x) %>% 
  spread_predictions(m_nopool,m_ppool)

p_data <- data %>% 
  ggplot(
    aes(x=x,y=y,color=sub)
  ) +
  geom_beeswarm(size=3) +
  theme_bw() +
  guides(color='none')+
  facet_grid(.~sub)

p_nopool <- data_pred %>% 
  ggplot(
    aes(x=x,y=m_nopool,color=sub)
  ) +
  stat_summary(fun='mean',geom='point')+
  stat_summary(geom='line',fun='mean',group='none')+
  theme_bw()+
  guides(color='none')+
  facet_grid(.~sub)

p_ppool <- data_pred %>% 
  ggplot(
    aes(x=x,y=m_ppool,color=sub)
  ) +
  stat_summary(fun='mean',geom='point')+
  stat_summary(geom='line',fun='mean',group='none')+
  theme_bw()+
  guides(color='none')+
  facet_grid(.~sub)

p_bayes_coefs <- data %>% 
  data_grid(sub,x) %>% 
  add_predicted_draws(no_pool_b, value="nopool") %>% 
  group_by(sub,x) %>% 
  median_hdi(.simple_names=F) %>% 
  add_predicted_draws(ppool_b, value="ppool") %>% 
  median_hdi(.simple_names=F) %>% 
  ggplot(
    aes(y=interaction(x,sub))
  ) +
  geom_point(aes(x=nopool),position=position_nudge(y=.1)) +
  geom_errorbar(aes(x=nopool,xmin=nopool.lower,xmax=nopool.upper),width=0,position=position_nudge(y=.1))+
  geom_point(aes(x=ppool),color='red',position=position_nudge(y=-.1))+
  geom_errorbar(aes(x=ppool,xmin=ppool.lower,xmax=ppool.upper),width=0,color='red',position=position_nudge(y=-.1))

p_data
```

##

```{r}
p_nopool
```

##
```{r}
p_ppool
```

## What's a mixed model?
![](mmmeme.jpeg){fig-align="center"}



## The linear model
$$ 
y = \alpha + \beta x + \epsilon 
$$

. . .

$$
Y = X\mathcal{B} + \epsilon
$$

. . .

$$ 
y \sim Normal(\mu, \sigma)\\
\mu = \alpha + \beta x\\
$$

$$
$$

## $y = \alpha + \epsilon$
```{r}
p1 <- data_1 %>% 
ggplot(aes(x=x1,y=y1))+
geom_point(size=3) +
theme_minimal() +
xlab('X') + 
ylab('Y')

p1
```

## $y = \alpha + \epsilon$
```r
lm(y ~ 1, data=df)
```
```{r}
p1 +
geom_smooth(method='lm', formula = y ~ 1)
```

## $y = \alpha + \beta x + \epsilon$ (continuous)
```{r}
p2 <- data_1 %>% 
ggplot(aes(x=x1,y=y2))+
geom_point(size=3) +
theme_minimal() +
xlab('X') + 
ylab('Y')

p2
```

## $y = \alpha + \beta x + \epsilon$ (continuous)

```r
lm(y ~ 1 + x, data=df)
```
```{r}
p2 + geom_smooth(method='lm', formula=y~1+x)
```

## $y = \alpha + \beta x + \epsilon$ (categorical)
```{r}
p3 <- data_1 %>% 
ggplot(aes(x=x2,y=y3))+
geom_point(size=3) +
theme_minimal() +
xlab('X') + 
ylab('Y') +
scale_x_continuous(breaks=c(0,1), limits=c(-0.5,1.5))

p3
```

## $y = \alpha + \beta x + \epsilon$ (categorical)

```r
lm(y ~ 1 + x, data=df)
```

```{r}
p3 + geom_smooth(method='lm')
```

## But sometimes our data is clustered...

:::{.incremental}
- within participants
- within stimuli types
- participants within groups
- measuring at different times
- measuring at different locations/by different people/using different materials
:::

## But sometimes our data is clustered...
```{r}
p4 <- data_2 %>% 
  ggplot(
    aes(
      x=x,y=y,color=factor(sub_id)
    )
  ) +
  geom_point() +
  theme_minimal() +
  guides(color='none', fill='none')

p4
```

## Full pooling

```r
lm(y ~ 1 + x, data=df)
```
```{r}
p4 +
geom_smooth(aes(group='none'),color='blue',method='lm',formula=y ~ 1 + x)
```

. . .

...ignores variability


## No pooling

```r
lm(y ~ 1 + x + subj + x:subj, data=df) #or y ~ 1 + x * subj
```

```{r}
p4 + geom_smooth(method='lm',se=F)
```

. . .

addresses variability, but...

## No pooling

:::{.incremental}
- less powerful: estimating a different intercept & slope for each group increases df
- likely to overfit to sample
- if clusters are of unequal size, some estimates are better informed than others
- sensitive to outliers
- unclear how results generalise to e.g., population
- *we can do better*
:::

## The solution: partial pooling

:::{.incremental}
:::

## ...Partial pooling
```{r}
p4 + geom_smooth(aes(y=pred),method='lm')
```

## Now it's your turn!

Open Rstudio!

## Load in packages

```r
library(tidyverse)
library(lme4)
library(emmeans)

set.seed(90095)
```

## Get data from my repo:

```r
data <- readRDS(url("http://tinyurl.com/mixedmodeldata"))
```

## Let's take a look at the data

```r
view(data)
```

## Let's take a look at the data

```{r}
kable(data)
```
